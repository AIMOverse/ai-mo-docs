"{\"value\":{\"mdx\":\"# Integration Examples\\n\\n## OpenAI Python Library Compatibility\\n\\nYou can use the OpenAI Python library with AiMo Network by setting a custom base URL:\\n\\n```python\\nimport openai\\n\\n# Configure for AiMo Network\\nopenai.api_base = \\\"https://devnet.aimo.network/api/v1\\\"\\nopenai.api_key = \\\"aimo-sk-dev-[your-api-key]\\\"\\n\\n# Use as normal, but specify provider:model format\\nresponse = openai.ChatCompletion.create(\\n    model=\\\"9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM:gpt-3.5-turbo\\\",\\n    messages=[\\n        {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello, AI!\\\"}\\n    ]\\n)\\n```\\n\\n## Langchain Integration\\n\\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\n\\n# Configure for AiMo Network\\nchat = ChatOpenAI(\\n    openai_api_base=\\\"https://devnet.aimo.network/api/v1\\\",\\n    openai_api_key=\\\"aimo-sk-dev-xxxxxxx\\\",\\n    model_name=\\\"9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM:gpt-3.5-turbo\\\"\\n)\\n\\nresponse = chat.predict(\\\"What is the future of AI?\\\")\\n```\\n\\n## Rate Limits and Best Practices\\n\\n### Performance Optimization\\n\\n1. **Use Streaming**: Enable streaming for better user experience\\n2. **Batch Requests**: Group multiple requests when possible\\n3. **Cache Responses**: Cache responses for repeated queries\\n4. **Monitor Costs**: Track token usage to optimize costs\\n\\n### Fair Usage\\n\\n1. **Respect Rate Limits**: Don't exceed provider-specific rate limits\\n2. **Optimize Prompts**: Use efficient prompts to reduce token costs\\n3. **Handle Errors Gracefully**: Implement proper retry logic with exponential backoff\\n\",\"document\":[{\"href\":\"/user-guide/integrations#integration-examples\",\"html\":\"</header>\\n\",\"id\":\"docs/pages/user-guide/integrations.mdx#integration-examples\",\"isPage\":true,\"text\":\"\\n\",\"title\":\"Integration Examples\",\"titles\":[]},{\"href\":\"/user-guide/integrations#openai-python-library-compatibility\",\"html\":\"\\n<p>You can use the OpenAI Python library with AiMo Network by setting a custom base URL:</p>\\n<pre class=\\\"shiki shiki-themes github-light github-dark-dimmed\\\" style=\\\"background-color:#fff;--shiki-dark-bg:#22272e;color:#24292e;--shiki-dark:#adbac7\\\" tabindex=\\\"0\\\"><code><span class=\\\"line\\\"><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">import</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\"> openai</span></span>\\n<span class=\\\"line\\\" data-empty-line=\\\"true\\\"> </span>\\n<span class=\\\"line\\\"><span style=\\\"color:#6A737D;--shiki-dark:#768390\\\"># Configure for AiMo Network</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">openai.api_base </span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\"> &quot;https://devnet.aimo.network/api/v1&quot;</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">openai.api_key </span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\"> &quot;aimo-sk-dev-[your-api-key]&quot;</span></span>\\n<span class=\\\"line\\\" data-empty-line=\\\"true\\\"> </span>\\n<span class=\\\"line\\\"><span style=\\\"color:#6A737D;--shiki-dark:#768390\\\"># Use as normal, but specify provider:model format</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">response </span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\"> openai.ChatCompletion.create(</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#E36209;--shiki-dark:#F69D50\\\">    model</span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM:gpt-3.5-turbo&quot;</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">,</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#E36209;--shiki-dark:#F69D50\\\">    messages</span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">[</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">        {</span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;role&quot;</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">: </span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;user&quot;</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">, </span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;content&quot;</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">: </span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;Hello, AI!&quot;</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">}</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">    ]</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">)</span></span></code></pre>\\n\",\"id\":\"docs/pages/user-guide/integrations.mdx#openai-python-library-compatibility\",\"isPage\":false,\"text\":\"\\nYou can use the OpenAI Python library with AiMo Network by setting a custom base URL:\\nimport openai\\n \\n# Configure for AiMo Network\\nopenai.api_base = &quot;https://devnet.aimo.network/api/v1&quot;\\nopenai.api_key = &quot;aimo-sk-dev-[your-api-key]&quot;\\n \\n# Use as normal, but specify provider:model format\\nresponse = openai.ChatCompletion.create(\\n    model=&quot;9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM:gpt-3.5-turbo&quot;,\\n    messages=[\\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello, AI!&quot;}\\n    ]\\n)\\n\",\"title\":\"OpenAI Python Library Compatibility\",\"titles\":[\"Integration Examples\"]},{\"href\":\"/user-guide/integrations#langchain-integration\",\"html\":\"\\n<pre class=\\\"shiki shiki-themes github-light github-dark-dimmed\\\" style=\\\"background-color:#fff;--shiki-dark-bg:#22272e;color:#24292e;--shiki-dark:#adbac7\\\" tabindex=\\\"0\\\"><code><span class=\\\"line\\\"><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">from</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\"> langchain.llms </span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">import</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\"> OpenAI</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">from</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\"> langchain.chat_models </span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">import</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\"> ChatOpenAI</span></span>\\n<span class=\\\"line\\\" data-empty-line=\\\"true\\\"> </span>\\n<span class=\\\"line\\\"><span style=\\\"color:#6A737D;--shiki-dark:#768390\\\"># Configure for AiMo Network</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">chat </span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\"> ChatOpenAI(</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#E36209;--shiki-dark:#F69D50\\\">    openai_api_base</span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;https://devnet.aimo.network/api/v1&quot;</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">,</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#E36209;--shiki-dark:#F69D50\\\">    openai_api_key</span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;aimo-sk-dev-xxxxxxx&quot;</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">,</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#E36209;--shiki-dark:#F69D50\\\">    model_name</span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM:gpt-3.5-turbo&quot;</span></span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">)</span></span>\\n<span class=\\\"line\\\" data-empty-line=\\\"true\\\"> </span>\\n<span class=\\\"line\\\"><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">response </span><span style=\\\"color:#D73A49;--shiki-dark:#F47067\\\">=</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\"> chat.predict(</span><span style=\\\"color:#032F62;--shiki-dark:#96D0FF\\\">&quot;What is the future of AI?&quot;</span><span style=\\\"color:#24292E;--shiki-dark:#ADBAC7\\\">)</span></span></code></pre>\\n\",\"id\":\"docs/pages/user-guide/integrations.mdx#langchain-integration\",\"isPage\":false,\"text\":\"\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\n \\n# Configure for AiMo Network\\nchat = ChatOpenAI(\\n    openai_api_base=&quot;https://devnet.aimo.network/api/v1&quot;,\\n    openai_api_key=&quot;aimo-sk-dev-xxxxxxx&quot;,\\n    model_name=&quot;9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM:gpt-3.5-turbo&quot;\\n)\\n \\nresponse = chat.predict(&quot;What is the future of AI?&quot;)\\n\",\"title\":\"Langchain Integration\",\"titles\":[\"Integration Examples\"]},{\"href\":\"/user-guide/integrations#rate-limits-and-best-practices\",\"html\":\"\\n\",\"id\":\"docs/pages/user-guide/integrations.mdx#rate-limits-and-best-practices\",\"isPage\":false,\"text\":\"\\n\",\"title\":\"Rate Limits and Best Practices\",\"titles\":[\"Integration Examples\"]},{\"href\":\"/user-guide/integrations#performance-optimization\",\"html\":\"\\n<ol>\\n<li><strong>Use Streaming</strong>: Enable streaming for better user experience</li>\\n<li><strong>Batch Requests</strong>: Group multiple requests when possible</li>\\n<li><strong>Cache Responses</strong>: Cache responses for repeated queries</li>\\n<li><strong>Monitor Costs</strong>: Track token usage to optimize costs</li>\\n</ol>\\n\",\"id\":\"docs/pages/user-guide/integrations.mdx#performance-optimization\",\"isPage\":false,\"text\":\"\\n\\nUse Streaming: Enable streaming for better user experience\\nBatch Requests: Group multiple requests when possible\\nCache Responses: Cache responses for repeated queries\\nMonitor Costs: Track token usage to optimize costs\\n\\n\",\"title\":\"Performance Optimization\",\"titles\":[\"Integration Examples\",\"Rate Limits and Best Practices\"]},{\"href\":\"/user-guide/integrations#fair-usage\",\"html\":\"\\n<ol>\\n<li><strong>Respect Rate Limits</strong>: Don&#x27;t exceed provider-specific rate limits</li>\\n<li><strong>Optimize Prompts</strong>: Use efficient prompts to reduce token costs</li>\\n<li><strong>Handle Errors Gracefully</strong>: Implement proper retry logic with exponential backoff</li>\\n</ol>\",\"id\":\"docs/pages/user-guide/integrations.mdx#fair-usage\",\"isPage\":false,\"text\":\"\\n\\nRespect Rate Limits: Don&#x27;t exceed provider-specific rate limits\\nOptimize Prompts: Use efficient prompts to reduce token costs\\nHandle Errors Gracefully: Implement proper retry logic with exponential backoff\\n\",\"title\":\"Fair Usage\",\"titles\":[\"Integration Examples\",\"Rate Limits and Best Practices\"]}]}}"
